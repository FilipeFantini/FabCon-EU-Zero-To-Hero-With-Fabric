{"cells":[{"cell_type":"markdown","source":["# Data Engineering with Spark\n","This notebook serves as Part 3 of Lab 4: Data Engineering in Fabric Notebooks. The goal is to demonstrate the foundational steps of data engineering using PySpark, leading to the creation of a Delta tables in the target Lakehouse. We will explore different methods of reading data into a DataFrame and how to transform this data effectively."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e137e07c-3013-4604-9363-4dfd30eff83d"},{"cell_type":"markdown","source":["## Querying Data using Spark SQL and PySpark\n","\n","This section demonstrates how to query data from tables using Spark SQL and PySpark. By utilizing Spark SQL, users can easily run SQL queries on their data directly in Spark. This method is highly beneficial for those familiar with SQL syntax and allows seamless interaction with data stored in DataFrames or tables within the Spark ecosystem.\n","\n","### SQL Query in Spark-SQL\n","\n","You can directly execute SQL queries using a magic command (%%sql) or by setting the cell to SQL. This approach does not require prior DataFrame registration. Without assigning the results to a dataframe or creating a temp view this approach is primarily for data exploration and profiling exercises."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ce5b555-9ce6-4ac3-a525-8ed6149ab0f4"},{"cell_type":"code","source":["%%sql\n","SELECT \n","    PackageTypeId,\n","    PackageTypeName,\n","    LastEditedBy,\n","    ValidFrom,\n","    ValidTo\n","FROM\n","    bronze_lakehouse_wtc.package_types -- change lakehouse reference"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"06fed711-5782-4bc6-9fe6-cccb519d82b8"},{"cell_type":"markdown","source":["### SQL Query in Spark-SQL\n","\n","Creating a temp view commits the results of the Spark-SQL query to memory making them available for use elsewhere in the notebook. Temp views aren't persisted and are dropped when the session terminates."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26ef0a6a-d653-47af-8bf2-4795baa53911"},{"cell_type":"code","source":["%%sql\n","-- Create temp view\n","CREATE OR REPLACE TEMP VIEW tmp_vw_package_types AS\n","SELECT \n","    PackageTypeId,\n","    PackageTypeName,\n","    LastEditedBy,\n","    ValidFrom,\n","    ValidTo\n","FROM\n","    bronze_lakehouse_wtc.package_types; -- change lakehouse reference\n","\n","-- Query view\n","SELECT *\n","FROM tmp_vw_package_types"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"7e8f5ae5-c321-4946-928b-2e9546aacb60"},{"cell_type":"markdown","source":["### Using Variables\n","\n","##### Using variables as part of your development helps to create dynamic data processing patterns. Set the lakehouse variables below before progressing."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fd449e6-8c33-41e0-8b9f-22558e59df58"},{"cell_type":"code","source":["# Assign bronze lakehouse variable value (e.g. bronze_lakehouse = 'bronze_lakehouse_wtc')\n","bronze_lakehouse = 'bronze_lakehouse_wtc'\n","\n","# Assign silver lakehouse variable value (e.g. silver_lakehouse = 'silver_lakehouse_wtc')\n","silver_lakehouse = 'silver_lakehouse_wtc'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef0618eb-b883-4be8-88be-240bceb2cda8"},{"cell_type":"markdown","source":["## Reading Data into DataFrames\n","\n","This section illustrates different methods to read data into a Spark DataFrame. Each method, while yielding the same result, offers different approaches that can be utilized based on the specific requirements of your data processing task.\n","\n","### SQL Query Execution in PySpark\n","\n","Here, we use Spark SQL to load data into a DataFrame. This method is particularly useful if you are comfortable with SQL syntax. It allows you to leverage the power of SQL queries within the Spark environment."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d39b4976-389f-4993-98d0-6752c0f3f095"},{"cell_type":"code","source":["# Use spark.sql to execute SQL queries\n","df_package_types = spark.sql(f\"\"\"\n","    SELECT \n","        PackageTypeId       package_type_id,\n","        PackageTypeName     package_type_name,\n","        LastEditedBy        last_edited_by,\n","        ValidFrom           valid_from,\n","        ValidTo             valid_to\n","    FROM\n","        {bronze_lakehouse}.package_types  -- Uses variable reference\n","\"\"\")\n","\n","# Display the result\n","display(df_package_types)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b9a23ddb-dbc5-4cf0-9d6e-1db68e1b9829"},{"cell_type":"markdown","source":["### Using PySpark DataFrame API\n","Alternatively, we can use the PySpark DataFrame API to achieve the same result. This approach is more native to Spark and utilizes the DataFrame API's methods for data manipulation."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cc6889fd-a8e2-4462-bb94-bae7d1413948"},{"cell_type":"code","source":["# Load the data into a DataFrame using PySpark DataFrame API\n","df = spark.table(f\"{bronze_lakehouse}.package_types\")  # Uses variable reference\n","\n","# Show the DataFrame\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"04fc3eb2-c05a-4db9-9b60-3dac9907b89a"},{"cell_type":"markdown","source":["## Writing the Package Types DataFrame to Silver Table\n","After creating a dataframe, we can write the dataframe to a delta table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ddd63adc-3e1a-4dfa-8651-dad19371c71e"},{"cell_type":"code","source":["# Set target table path\n","target_table = f'{silver_lakehouse}.package_types'  # Uses variable reference\n","\n","df_package_types.write.format('delta').mode('overwrite').option(\"overwriteSchema\", True).saveAsTable(target_table)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ae15b1a-7b56-471d-b6fd-10092cd823e9"},{"cell_type":"markdown","source":["## Assigning Explicit Schema and Aliasing Fields using DataFrame API\n","\n","In this section, we define an explicit schema for a Delta table, use the PySpark DataFrame API to load the data, and apply aliasing the columns."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1489294c-f6f2-41df-957e-d67212fe3c02"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n","\n","# Define the schema for the locations delta table\n","locations_schema = StructType([\n","    StructField(\"StateProvinceID\", IntegerType(), True),\n","    StructField(\"StateProvinceCode\", StringType(), True),\n","    StructField(\"StateProvinceName\", StringType(), True),\n","    StructField(\"SalesTerritory\", StringType(), True),\n","    StructField(\"CityID\", IntegerType(), True),\n","    StructField(\"CityName\", StringType(), True)\n","])\n","\n","# Read the Delta table into a DataFrame using the explicit schema\n","df_locations = spark.table(f'{bronze_lakehouse}.locations')   # Uses variable reference\n","\n","# Alias the columns with lower-snake_case\n","df_aliased = df_locations.select(\n","    df_locations[\"StateProvinceID\"].alias(\"state_province_id\"),\n","    df_locations[\"StateProvinceCode\"].alias(\"state_province_code\"),\n","    df_locations[\"StateProvinceName\"].alias(\"state_province_name\"),\n","    df_locations[\"SalesTerritory\"].alias(\"sales_territory\"),\n","    df_locations[\"CityID\"].alias(\"city_id\"),\n","    df_locations[\"CityName\"].alias(\"city_name\")\n",")\n","\n","# Show the aliased DataFrame\n","# display(df_aliased)\n","\n","# Write silver table\n","df_aliased.write.format('delta').mode('overwrite').saveAsTable(f'{silver_lakehouse}.locations')   # Uses variable reference"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a4da9dc6-a1c0-4512-9216-97720afaa199"},{"cell_type":"markdown","source":["### This concludes the notebook portion of this lab."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"efd86dea-2544-491a-b3ff-d66432e01c04"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}